{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4e7d7d-77c6-494e-ab3a-2f7bd431a2d3",
   "metadata": {},
   "source": [
    "Instructions (Please read carefully):\n",
    "\n",
    "Requirements: \n",
    "Python>=3.0\n",
    "Pandas>=2.0\n",
    "\n",
    "This code is for compiling the data from each sample from Kubios HRV Scientific to a CSV file. You can select the sample size and how many samples you want to compile. If you are here, that means you know how difficult compiling them is manually without having the pro version. Let me make your life easier for you. Read the instructions below carefully before proceeding:\n",
    "\n",
    "This works by taking the data from the results that can be saved using kubios to a CSV file. The code does not sample or extract any data from the Kubios software. You can save the results of a sample in Kubios as a CSV (Ensure you are analysing the correct timestamp -> File -> Save result as -> save the file in CSV format). PLEASE NAME EACH FILE WITH THE CONTEXT FOR RECOGNITION (eg, Ibrahim zone 1 day 1, Chayan zone 1 day 2, Abhay zone 2 day 1, etc.)\n",
    "\n",
    "COMPILE ALL THE CSVs IN ONE FOLDER WITHOUT ANY OTHER FILES\n",
    "COPY THE FILE PATH (ctrl + shift + c)\n",
    "PASTE IT AT THE INPUT \n",
    "And voila, all the data samples have been compiled into one CSV. Hope this save you time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f7eb58-5958-449e-a36e-dddf7b7584bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "\n",
    "path_to_csv = input(print(\"Input Folder with the CSV: \")).strip('\"')\n",
    "\n",
    "path_to_csv = path_to_csv.replace(\"\\\\\",\"/\")\n",
    "\n",
    "print(\"Using path: \", path_to_csv)\n",
    "\n",
    "if os.path.exists(path_to_csv):\n",
    "    print(\"\\033[92m Path is valid!\")\n",
    "else:\n",
    "    print(\"\\033[91m Path not found!\")\n",
    "\n",
    "csv_files = glob.glob(os.path.join(path_to_csv, \"*.csv\"))\n",
    "\n",
    "for files in csv_files:\n",
    "    if os.path.exists(files):\n",
    "        continue\n",
    "    else:\n",
    "        print(files, \"\\033[91m This is path is not found\")\n",
    "print(\"\\033[92m All files are valid\")\n",
    "\n",
    "out_folder = os.path.join(path_to_csv, \"with headers\")\n",
    "\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "print(\"Creating new folder 'with headers'\")\n",
    "\n",
    "header = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\"]\n",
    "\n",
    "for file in os.listdir(path_to_csv):\n",
    "    if file.endswith(\".csv\"):\n",
    "        in_path = os.path.join(path_to_csv, file)\n",
    "\n",
    "        with open(in_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            rows = list(csv.reader(f))\n",
    "\n",
    "\n",
    "        \n",
    "        out_path = os.path.join(out_folder, file)\n",
    "        with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header)   # add header\n",
    "            writer.writerows(rows)    # add original content\n",
    "\n",
    "\n",
    "csv_files = glob.glob(os.path.join(out_folder, \"*.csv\"))\n",
    "\n",
    "for files in csv_files:\n",
    "    if os.path.exists(files):\n",
    "        continue\n",
    "    else:\n",
    "        print(files, \"\\033[91mthis is path is not found\")\n",
    "\n",
    "print(\"\\033[92m All the new files are valid\")\n",
    "\n",
    "file_name = []\n",
    "timestamp = []\n",
    "mean_RR = []\n",
    "SDNN = []\n",
    "Mean_HR = []\n",
    "STD_HR = []\n",
    "Min_HR = []\n",
    "Max_HR = []\n",
    "RMSSD = []\n",
    "NNxx = []\n",
    "pNNxx = []\n",
    "HRV_Index = []\n",
    "TINN = []\n",
    "Stress_Index = []\n",
    "LF_Power = []\n",
    "LF_Power_log = []\n",
    "LF_Power_percentage = []\n",
    "LF_Power_nu = []\n",
    "HF_Power = []\n",
    "HF_Power_log = []\n",
    "HF_Power_percentage = []\n",
    "HF_Power_nu = []\n",
    "LF_HF_ratio = []\n",
    "SD1 = []\n",
    "SD2 = []\n",
    "SD2_SD1 = []\n",
    "ApEn = []\n",
    "SampEn = []\n",
    "DFA_alpha1 = []\n",
    "DFA_alpha2 = []\n",
    "\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    file_name.append(os.path.basename(file))\n",
    "    timestamp.extend(df.loc[df[\"1\"] == \"  Sample limits (hh:mm:ss):   \", \"2\"].tolist())\n",
    "    mean_RR.extend(df.loc[df[\"1\"] == \"  Mean RR  (ms):              \", \"2\"].tolist())\n",
    "    SDNN.extend(df.loc[df[\"1\"] == \"  SDNN (ms):                  \", \"2\"].tolist())\n",
    "    Mean_HR.extend(df.loc[df[\"1\"] == \"  Mean HR (beats/min):        \", \"2\"].tolist())\n",
    "    STD_HR.extend(df.loc[df[\"1\"] == \"  SD HR (beats/min):          \", \"2\"].tolist())\n",
    "    Min_HR.extend(df.loc[df[\"1\"] == \"  Min HR (beats/min):         \", \"2\"].tolist())\n",
    "    Max_HR.extend(df.loc[df[\"1\"] == \"  Max HR (beats/min):         \", \"2\"].tolist())\n",
    "    RMSSD.extend(df.loc[df[\"1\"] == \"  RMSSD (ms):                 \", \"2\"].tolist())\n",
    "    NNxx.extend(df.loc[df[\"1\"] == \"  NNxx (beats):               \", \"2\"].tolist())\n",
    "    pNNxx.extend(df.loc[df[\"1\"] == \"  pNNxx (%):                  \", \"2\"].tolist())\n",
    "    HRV_Index.extend(df.loc[df[\"1\"] == \"  RR tri index:               \", \"2\"].tolist())\n",
    "    TINN.extend(df.loc[df[\"1\"] == \"  TINN (ms):                  \", \"2\"].tolist())\n",
    "    Stress_Index.extend(df.loc[df[\"1\"] == \"  Stress index:               \", \"2\"].tolist())\n",
    "    LF_Power.extend(df.loc[df[\"1\"] == \"  LF (ms^2):                  \", \"2\"].tolist())\n",
    "    LF_Power_log.extend(df.loc[df[\"1\"] == \"  LF (log):                   \", \"2\"].tolist())\n",
    "    LF_Power_percentage.extend(df.loc[df[\"1\"] == \"  LF (%):                     \", \"2\"].tolist())\n",
    "    LF_Power_nu.extend(df.loc[df[\"1\"] == \"  LF (n.u.):                  \", \"2\"].tolist())\n",
    "    HF_Power.extend(df.loc[df[\"1\"] == \"  HF (ms^2):                  \", \"2\"].tolist())\n",
    "    HF_Power_log.extend(df.loc[df[\"1\"] == \"  HF (log):                   \", \"2\"].tolist())\n",
    "    HF_Power_percentage.extend(df.loc[df[\"1\"] == \"  HF (%):                     \", \"2\"].tolist())\n",
    "    HF_Power_nu.extend(df.loc[df[\"1\"] == \"  HF (n.u.):                  \", \"2\"].tolist())\n",
    "    LF_HF_ratio.extend(df.loc[df[\"1\"] == \" LF/HF ratio:                 \", \"2\"].tolist())\n",
    "    SD1.extend(df.loc[df[\"1\"] == \"  SD1 (ms):                   \", \"2\"].tolist())\n",
    "    SD2.extend(df.loc[df[\"1\"] == \"  SD2 (ms):                   \", \"2\"].tolist())\n",
    "    SD2_SD1.extend(df.loc[df[\"1\"] == \"  SD2/SD1 ratio:              \", \"2\"].tolist())\n",
    "    ApEn.extend(df.loc[df[\"1\"] == \" Approximate entropy (ApEn):  \", \"2\"].tolist())\n",
    "    SampEn.extend(df.loc[df[\"1\"] == \" Sample entropy (SampEn):     \", \"2\"].tolist())\n",
    "    DFA_alpha1.extend(df.loc[df[\"1\"] == \"  alpha 1:                    \", \"2\"].tolist())\n",
    "    DFA_alpha2.extend(df.loc[df[\"1\"] == \"  alpha 2:                    \", \"2\"].tolist())\n",
    "\n",
    "all_var = {\n",
    "    \"File Name\": file_name,\n",
    "    \"Timestamp of the sample\" :timestamp,\n",
    "    \"Mean RR\" : mean_RR, \"SDNN\": SDNN, \"Mean HR\" :Mean_HR, \"STD HR\" : STD_HR,\"Min HR\" :Min_HR, \"Max HR\": Max_HR, \"RMSSD\":RMSSD, \"NNxx\":NNxx,\n",
    "    \"pNNxx\": pNNxx,\"HRV Triangular Index\" : HRV_Index, \"TINN\": TINN, \"Stress Index\" :Stress_Index, \"LF Power(ms^2)\": LF_Power, \"LF Power (log)\":LF_Power_log,\n",
    "    \"LF Power %\": LF_Power_percentage, \"LF Power(n.u)\": LF_Power_nu, \"HF Power(ms^2)\": HF_Power, \"HF Power (log)\":HF_Power_log,\n",
    "    \"HF Power %\": HF_Power_percentage, \"HF Power(n.u)\": HF_Power_nu, \"LF/HF ratio\": LF_HF_ratio, \"SD1\" :SD1, \"SD2\": SD2, \"SD2/SD1\": SD2_SD1, \"ApEn\": ApEn,\n",
    "    \"Sample Entropy\": SampEn, \"DFA alpha 1\" :DFA_alpha1, \"DFA alpha 2\": DFA_alpha2}\n",
    "\n",
    "check_lens = [len(list) for names, list in all_var.items()]\n",
    "\n",
    "if len(set(check_lens)) == 1:\n",
    "    print(\"\\033[92m All Data available\")\n",
    "else:\n",
    "    for name, var in all_var.items():\n",
    "        print(f\"\\033[91m{name}: {len(var)}\")\n",
    "    print(\"\\033[91m Missing data or blank data, make sure all the data is present in the new folder\")\n",
    "\n",
    "df = pd.DataFrame(all_var)\n",
    "\n",
    "save = input(\"Save File as: \")\n",
    "df.to_csv(save+\".csv\")\n",
    "\n",
    "print(f\"\\033[90m File Saved as {save}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273388f-81ef-472b-b973-bac789797537",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
